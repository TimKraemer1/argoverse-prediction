{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Specific Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Other\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "from DatasetClass import TrajectoryDatasetTest, TrajectoryDatasetTrain\n",
    "from models.LSTMModelClass import LSTMModel\n",
    "from models.LinearRegressionModelClass import LinearRegressionModel\n",
    "from models.CNNModelClass import CNNModel\n",
    "from models.MLPModelClass import MLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50, 110, 6) (2100, 50, 50, 6)\n",
      "(10000, 50, 50, 6) (10000, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('cse-251-b-2025/input/train.npz')['data']\n",
    "test_data  = np.load('cse-251-b-2025/input/test_input.npz')['data']\n",
    "\n",
    "X_train = train_data[..., :50, :] # Input (scenes, agents, first 50 timesteps, features)\n",
    "Y_train = train_data[:, 0, 50:, :2] # Output (scenes, ego car, next 60 timesteps, x/y coordinates)\n",
    "\n",
    "# Print input and output data shape\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU\n",
      "torch.Size([5, 50, 6])\n"
     ]
    }
   ],
   "source": [
    "# Used from midquarter checkpoint for simplicity and consistency\n",
    "torch.manual_seed(251)\n",
    "np.random.seed(42)\n",
    "\n",
    "scale = 7.0\n",
    "\n",
    "N = len(train_data)\n",
    "val_size = int(0.1 * N)\n",
    "train_size = N - val_size\n",
    "\n",
    "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "device = torch.device('mps')\n",
    "print(\"Using Apple Silicon GPU\")\n",
    "\n",
    "print(train_dataset[0].x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'LSTM'\n",
    "match model_type:\n",
    "    case 'LinearRegression':\n",
    "        model = LinearRegressionModel().to(device)\n",
    "    case 'MLP':\n",
    "        model = MLPModel(6*50, 60 * 2).to(device)\n",
    "    case 'CNN':\n",
    "        model = CNNModel().to(device)\n",
    "    case 'LSTM':\n",
    "        model = LSTMAttention().to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "# scheduler\n",
    "step = 20\n",
    "gamma = 0.25\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step, gamma)\n",
    "\n",
    "\n",
    "early_stopping_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "no_improvement = 0\n",
    "\n",
    "# Criterion MSE\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mae_list = []\n",
    "val_mse_list = []\n",
    "for epoch in tqdm.tqdm(range(200), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mae = 0\n",
    "    val_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            val_loss += criterion(pred, y).item()\n",
    "\n",
    "            # show MAE and MSE with unnormalized data\n",
    "            pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            y = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            val_mae += nn.L1Loss()(pred, y).item()\n",
    "            val_mse += nn.MSELoss()(pred, y).item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "    val_mse /= len(val_dataloader)\n",
    "    # scheduler.step()\n",
    "    # scheduler.step(val_loss)\n",
    "\n",
    "    val_mae_list.append(val_mae)\n",
    "    val_mse_list.append(val_mse)\n",
    "    \n",
    "    tqdm.tqdm.write(f\"Epoch {epoch:03d} | Learning rate {optimizer.param_groups[0]['lr']:.6f} | train normalized MSE {train_loss:8.4f} | val normalized MSE {val_loss:8.4f}, | val MAE {val_mae:8.4f} | val MSE {val_mse:8.4f}\")\n",
    "    if val_loss < best_val_loss - 1e-3:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement == early_stopping_patience // 2:\n",
    "            print(\"Learning rate decay\")\n",
    "            scheduler.step()\n",
    "        if no_improvement >= early_stopping_patience:\n",
    "            print(\"Early stop!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MSE and MAE\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(val_mse_list, label='Validation MSE')\n",
    "plt.plot(val_mae_list, label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('LSTM Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Plotting, used from midquarter checkpoint\n",
    "\n",
    "def plot_trajectory(ax, pred, gt, title=None):\n",
    "    ax.cla()\n",
    "    # Plot the predicted future trajectory\n",
    "    ax.plot(pred[0,:60,0], pred[0,:60,1], color='red', label='Predicted Future Trajectory')\n",
    "    \n",
    "    # Plot the ground truth future trajectory\n",
    "    ax.plot(gt[0,:60,0], gt[0,:60,1], color='blue', label='Ground Truth Future Trajectory')\n",
    "    \n",
    "    # Optionally set axis limits, labels, and title.\n",
    "    x_max = max(pred[..., 0].max(), gt[..., 0].max())\n",
    "    x_min = min(pred[..., 0].min(), gt[..., 0].min())\n",
    "    y_max = max(pred[..., 1].max(), gt[..., 1].max())\n",
    "    y_min = min(pred[..., 1].min(), gt[..., 1].min())\n",
    "    \n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel('X-axis')\n",
    "    ax.set_ylabel('Y-axis')\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# randomly select 4 samples from the validation set\n",
    "random_indices = random.sample(range(len(val_dataset)), 4)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axes = axes.flatten()  # Flatten the array to iterate single axes objects\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    batch = val_dataset[idx]\n",
    "    batch = batch.to(device)\n",
    "    pred = model(batch)\n",
    "    gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0)\n",
    "\n",
    "    pred = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "    gt = torch.stack(torch.split(batch.y, 60, dim=0), dim=0) * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    gt = gt.detach().cpu().numpy()\n",
    "\n",
    "    # Plot the trajectory using the i-th axis\n",
    "    plot_trajectory(axes[i], pred, gt, title=f\"Sample {idx}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "test_dataset = TrajectoryDatasetTest(test_data, scale=scale)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                         collate_fn=lambda xs: Batch.from_data_list(xs))\n",
    "\n",
    "best_model = torch.load(\"best_model.pt\")\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        pred_norm = model(batch)\n",
    "        \n",
    "        # Reshape the prediction to (N, 60, 2)\n",
    "        pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
    "\n",
    "        # Apply Gaussian smoothing along the time dimension (axis=1)\n",
    "        pred_numpy = pred.cpu().numpy()\n",
    "        pred_smoothed = np.zeros_like(pred_numpy)\n",
    "\n",
    "        #Smooth x and y\n",
    "        pred_smoothed[:, :, 0] = gaussian_filter1d(pred_numpy[:, :, 0], sigma=0.8, axis=1)\n",
    "        pred_smoothed[:, :, 1] = gaussian_filter1d(pred_numpy[:, :, 1], sigma=0.8, axis=1)\n",
    "        \n",
    "        pred_list.append(pred_smoothed)\n",
    "        \n",
    "pred_list = np.concatenate(pred_list, axis=0)  # (N,60,2)\n",
    "pred_output = pred_list.reshape(-1, 2)  # (N*60, 2)\n",
    "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "output_df.index.name = 'index'\n",
    "output_df.to_csv('submission06-01-2025.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
